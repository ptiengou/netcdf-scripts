{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setting up viz\n",
    "\n",
    "NB : conda lam1env (Python3.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#import personnal tools\n",
    "import sys\n",
    "sys.path.append('../../python_tools/')\n",
    "from tools import *\n",
    "from tools_mapping import *\n",
    "from tools_hf import *\n",
    "from tools_native import *\n",
    "from tools_LIAISE import *\n",
    "from tools_mesoNH import *\n",
    "\n",
    "# sys.path.append('../../python_tools/aborella/PLOTS/')\n",
    "# from datasets import *\n",
    "sys.path.append('../../python_tools/aborella/UTIL/')\n",
    "import xr_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy.rcParams['auto_show'] = True\n",
    "mpl.rcParams['figure.figsize'] = [10., 8.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model files and edit datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesoNH_dir = '../../../mesoNH_simulations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_MesoNH_outputs_basic(filename, ds_name=None):\n",
    "    ds = xr.open_mfdataset(filename)\n",
    "    ds.attrs['name']= ds_name\n",
    "\n",
    "    rename_dict={\n",
    "        'time':'start_time',\n",
    "        'longitude':'lon',\n",
    "        'latitude':'lat'\n",
    "    }\n",
    "    ds = ds.rename(rename_dict)\n",
    "\n",
    "    # Add time coordinate\n",
    "    # Assuming 'time' coordinate is the starting time.\n",
    "    base_time = pd.to_datetime(ds[\"start_time\"].isel(start_time=0).item())\n",
    "    print(f\"Base time for the dataset: {base_time}\")\n",
    "    # Create a timedelta for each hour\n",
    "    hourly_offsets = pd.to_timedelta(ds[\"record\"].values, unit='h')\n",
    "    new_time_vals = base_time + hourly_offsets\n",
    "    # Assign new_time as a new coordinate, linked to the 'record' dimension\n",
    "    ds = ds.assign_coords(time=(\"record\", new_time_vals))\n",
    "    ds = ds.swap_dims({\"record\": \"time\"})\n",
    "    #drop start_time and record\n",
    "    ds = ds.drop_vars(['start_time'])\n",
    "\n",
    "    if 'LE_ISBA' in ds:\n",
    "        ds = ds.rename({'LE_ISBA': 'flat'})\n",
    "        ds['flat'].attrs['long_name'] = 'Latent Heat Flux on natural areas'\n",
    "        ds['flat'].attrs['units'] = 'W m⁻²'\n",
    "    if 'H_ISBA' in ds:\n",
    "        ds = ds.rename({'H_ISBA': 'sens'})\n",
    "        ds['sens'].attrs['long_name'] = 'Sensible Heat Flux on natural areas'\n",
    "        ds['sens'].attrs['units'] = 'W m⁻²'\n",
    "    if 'HU2M_ISBA' in ds:\n",
    "        ds['rh2m'] = ds['HU2M_ISBA'] * 100.0 \n",
    "        ds['rh2m'].attrs['long_name'] = 'Relative Humidity at 2m'\n",
    "        ds['rh2m'].attrs['units'] = '%'\n",
    "    if 'T2M_ISBA' in ds:\n",
    "        ds = ds.rename({'T2M_ISBA': 't2m'})\n",
    "        ds['t2m'].attrs['long_name'] = 'Temperature at 2m'\n",
    "        ds['t2m'] = ds['t2m'] - 273.15\n",
    "        ds['t2m'].attrs['units'] = '°C'\n",
    "    if 'SWD' in ds:\n",
    "        ds = ds.rename({'SWD': 'SWdnSFC'})\n",
    "        ds['SWdnSFC'].attrs['long_name'] = 'Downward Shortwave Radiation at Surface'\n",
    "        ds['SWdnSFC'].attrs['units'] = 'W m⁻²'\n",
    "\n",
    "    return(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='{}/LIAIS.1*.nc'.format(mesoNH_dir)\n",
    "ds_orig = format_MesoNH_outputs_basic(filename)\n",
    "ds_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mesoNH_timestamp_restrict(ds, var, vmin=None, vmax=None, cmap='viridis',\n",
    "                         add_liaise=False,\n",
    "                         timestamp='2021-07-14T01:00:00',\n",
    "                         lon_min=None, lon_max=None, lat_min=None, lat_max=None):\n",
    "    # Select data for the given timestamp\n",
    "    data_to_plot_time_selected = ds[var].sel(time=timestamp, method='nearest')\n",
    "\n",
    "    # Get the 2D lon/lat coordinates from the dataset (they have nj, ni dimensions)\n",
    "    lons_coord = ds['lon']\n",
    "    lats_coord = ds['lat']\n",
    "\n",
    "    # Apply spatial subsetting if bounds are provided\n",
    "    if lon_min is not None and lon_max is not None and \\\n",
    "       lat_min is not None and lat_max is not None:\n",
    "\n",
    "        # Create a boolean mask based on the 2D lon/lat coordinates\n",
    "        # This mask will have dimensions (nj, ni)\n",
    "        spatial_mask = (lons_coord >= lon_min) & (lons_coord <= lon_max) & \\\n",
    "                       (lats_coord >= lat_min) & (lats_coord <= lat_max)\n",
    "\n",
    "        # Apply the mask to the data. Values outside the region will become NaN.\n",
    "        data_to_plot = data_to_plot_time_selected.where(spatial_mask)\n",
    "    else:\n",
    "        # If no subsetting is requested, use the time-selected data directly\n",
    "        data_to_plot = data_to_plot_time_selected\n",
    "        \n",
    "    title = f'{var} on {pd.to_datetime(timestamp).strftime(\"%Y-%m-%d %H:%M UTC\")}'\n",
    "    label = f'{var} ({ds[var].attrs.get(\"units\", \"\")})'\n",
    "\n",
    "    nice_map_mesoNH(data_to_plot, vmin=vmin, vmax=vmax, cmap=cmap,\n",
    "                         add_liaise=add_liaise, title=title, label=label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var='rh2m'\n",
    "vmin=0\n",
    "vmax=100\n",
    "# vmin,vmax=None, None\n",
    "cmap=blues\n",
    "map_mesoNH_timestamp_restrict(ds_orig, \n",
    "                     var,\n",
    "                     vmin=vmin, vmax=vmax, cmap=cmap,\n",
    "                     timestamp='2021-07-20T12:00:00',\n",
    "                     add_liaise=False,\n",
    "                    lon_min=-1, lon_max=1, lat_min=41, lat_max=42\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var='flat'\n",
    "vmin=0\n",
    "vmax=250\n",
    "# vmin,vmax=None, None\n",
    "cmap=blues\n",
    "map_mesoNH_mean(ds_orig, \n",
    "                 var,\n",
    "                 vmin=vmin, vmax=vmax, cmap=cmap,\n",
    "                 title='Latent Heat Flux mean (14-31/07/2021)',\n",
    "                 add_liaise=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var='flat'\n",
    "ds=ds_orig\n",
    "ds_list=[ds]\n",
    "\n",
    "time_series_ave(ds_list, var, ds_colors=False, title='{} {}'.format(var, ds[var].attrs['units']))\n",
    "# seasonal_cycle_ave(ds_list, var, ds_colors=True, year_min=year_min, year_max=year_max, ylabel=ylabel, title=title, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with LMDZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xesmf as xe\n",
    "\n",
    "def reformat_datasets_for_comparison(ds_source_initial, ds_target_rectilinear, var_names):\n",
    "    \"\"\"\n",
    "    Reformats two datasets for comparison:\n",
    "    1. Converts the source dataset (with record, nj, ni dimensions) to have a 'time' dimension\n",
    "       based on record index, and regrids it onto the target dataset's rectilinear grid.\n",
    "    2. Aligns the time dimension of both datasets.\n",
    "\n",
    "    Args:\n",
    "        ds_source_initial (xr.Dataset): The source dataset with LE_ISBA(record, nj, ni)\n",
    "                                        and 2D lat/lon coordinates (nj, ni).\n",
    "                                        Must have 'time' coordinate (record, datetime64)\n",
    "                                        and 'record' coordinate (record).\n",
    "        ds_target_rectilinear (xr.Dataset): The target dataset with LE_ISBA(time, lat, lon)\n",
    "                                            and 1D lat/lon dimensions.\n",
    "        var_names (list): A list of variable names (strings) to reformat and compare.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (ds_source_regridded_aligned, ds_target_aligned).\n",
    "               Both datasets will have (time, lat, lon) dimensions and aligned times.\n",
    "    \"\"\"\n",
    "    # Prepare ds_source_initial: add 'new_time' and swap 'record' dimension\n",
    "    base_time = pd.to_datetime(ds_source_initial[\"time\"].isel(record=0).item())\n",
    "    hourly_offsets = pd.to_timedelta(np.arange(ds_source_initial.sizes[\"record\"]), unit='H')\n",
    "    new_time_values = base_time + hourly_offsets\n",
    "    \n",
    "    ds_source_prepared = ds_source_initial.assign_coords(new_time=(\"record\", new_time_values))\n",
    "    ds_source_prepared = ds_source_prepared.swap_dims({'record': 'new_time'})\n",
    "    ds_source_prepared = ds_source_prepared.drop_vars('record')\n",
    "    ds_source_prepared = ds_source_prepared.rename_dims({'new_time': 'time'}) # Ensure time dim name consistency\n",
    "\n",
    "    # Initialize regridder for spatial transformation\n",
    "    regridder = xe.Regridder(ds_source_prepared, ds_target_rectilinear, method='bilinear', periodic=False)\n",
    "\n",
    "    # Regrid specified variables from ds_source\n",
    "    regridded_vars = {}\n",
    "    for var_name in var_names:\n",
    "        regridded_vars[var_name] = regridder(ds_source_prepared[var_name])\n",
    "\n",
    "    # Combine regridded variables into a new Dataset\n",
    "    ds_source_regridded = xr.Dataset(regridded_vars, coords=regridded_vars[var_names[0]].coords)\n",
    "\n",
    "    # Align time dimensions of both datasets for comparison\n",
    "    # Use reindex_like to ensure common time steps; fill with NaN if no match\n",
    "    ds_source_aligned = ds_source_regridded.reindex_like(ds_target_rectilinear, method='nearest', tolerance=pd.Timedelta('0.5 hours'))\n",
    "    ds_target_aligned = ds_target_rectilinear.reindex_like(ds_source_regridded, method='nearest', tolerance=pd.Timedelta('0.5 hours'))\n",
    "    \n",
    "    return ds_source_aligned, ds_target_aligned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lam1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
